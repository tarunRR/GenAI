{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89fde28",
   "metadata": {},
   "source": [
    "# Fine-tuning Phi-2 for Insurance Domain\n",
    "\n",
    "This notebook demonstrates fine-tuning Microsoft's Phi-2 model using **LoRA (Low-Rank Adaptation)** for insurance claims processing tasks. We'll train the model on instruction-response pairs to generate domain-specific explanations for claim denials and appeals.\n",
    "\n",
    "## Workflow Overview\n",
    "1. **Data Preparation**: Create and format insurance claims dataset\n",
    "2. **Model Setup**: Load Phi-2 and configure LoRA adapter\n",
    "3. **Training**: Fine-tune the model with optimized settings\n",
    "4. **Inference**: Generate responses for new queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Define training data: insurance claim instruction-response pairs\n",
    "data = [\n",
    "    {\n",
    "        \"instruction\": \"Explain why CPT 70553 was denied.\",\n",
    "        \"response\": \"Authorization denied because neurological deficit criteria required under Policy 4.12 were not documented.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a short appeal for denied MRI brain claim.\",\n",
    "        \"response\": \"We respectfully request reconsideration as the patient presented worsening neurological symptoms that meet medical necessity criteria.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain denial for incomplete prior authorization.\",\n",
    "        \"response\": \"Authorization denied due to missing clinical documentation supporting medical necessity.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to Hugging Face Dataset format for easy processing\n",
    "dataset = Dataset.from_list(data)\n",
    "print(f\"Dataset created with {len(dataset)} examples\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca171a8",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "### Create Insurance Claims Dataset\n",
    "We'll create a small dataset of instruction-response pairs focused on insurance claim denials and appeals. Each example contains:\n",
    "- **instruction**: The user's query about insurance matters\n",
    "- **response**: The model's expected response\n",
    "\n",
    "This dataset serves as the training data for fine-tuning Phi-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Specify the model to fine-tune\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "# Load the tokenizer - converts text to token IDs\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the pre-trained Phi-2 model - 2.7B parameter causal language model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"âœ… Loaded model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a3a00",
   "metadata": {},
   "source": [
    "## 2. Model Initialization\n",
    "\n",
    "### Load Phi-2 Model and Tokenizer\n",
    "We load Microsoft's Phi-2 model (2.7B parameters) and its tokenizer. This lightweight model is ideal for fine-tuning on consumer hardware while maintaining strong performance on domain-specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688dcfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    \"\"\"\n",
    "    Format a single example into the instruction-response prompt structure.\n",
    "    \n",
    "    Args:\n",
    "        example: Dict with 'instruction' and 'response' keys\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'text' key containing the formatted prompt\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"text\": f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "{example['response']}\"\"\"\n",
    "    }\n",
    "\n",
    "# Apply formatting to all dataset examples\n",
    "formatted_dataset = dataset.map(format_example)\n",
    "print(f\"Dataset formatted with {len(formatted_dataset)} examples\")\n",
    "formatted_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c7371",
   "metadata": {},
   "source": [
    "## 3. Dataset Formatting\n",
    "\n",
    "### Format Data for Language Modeling\n",
    "Convert each instruction-response pair into a formatted text string structured as:\n",
    "```\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{response}\n",
    "```\n",
    "\n",
    "This format helps the model understand the task structure during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2232f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA (Low-Rank Adaptation) parameters\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                                              # Rank of the low-rank matrices (lower = fewer parameters)\n",
    "    lora_alpha=16,                                    # Scaling factor for LoRA weights\n",
    "    target_modules=[\"Wqkv\", \"fc1\", \"fc2\"],            # Phi-2 transformer layers to apply LoRA to\n",
    "    lora_dropout=0.05,                                # Dropout for LoRA layers\n",
    "    bias=\"none\",                                       # No bias in LoRA modules\n",
    "    task_type=\"CAUSAL_LM\"                             # Task type: Causal Language Modeling\n",
    ")\n",
    "\n",
    "# Wrap the base model with LoRA adapters\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Display trainable parameters\n",
    "print(\"ðŸ”§ LoRA Configuration Applied:\")\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d4f44d",
   "metadata": {},
   "source": [
    "## 4. LoRA Configuration\n",
    "\n",
    "### Apply Low-Rank Adaptation\n",
    "**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning technique that:\n",
    "- Adds trainable low-rank matrices to attention layers\n",
    "- Reduces the number of trainable parameters from millions to thousands\n",
    "- Enables fine-tuning on consumer hardware without storing full model weights\n",
    "\n",
    "We configure LoRA for Phi-2's specific architecture targeting the attention and feed-forward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b94c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 1/9 [01:16<10:14, 76.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.3174, 'learning_rate': 0.00017777777777777779, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 2/9 [02:08<07:14, 62.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.7894, 'learning_rate': 0.00015555555555555556, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [02:09<03:24, 34.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.1808, 'learning_rate': 0.00013333333333333334, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [02:10<01:44, 20.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.8009, 'learning_rate': 0.00011111111111111112, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [02:10<00:54, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.153, 'learning_rate': 8.888888888888889e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [02:11<00:27,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.4323, 'learning_rate': 6.666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [02:11<00:12,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.5497, 'learning_rate': 4.4444444444444447e-05, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [02:12<00:04,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.3521, 'learning_rate': 2.2222222222222223e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [02:13<00:00, 14.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.7689, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "{'train_runtime': 133.2259, 'train_samples_per_second': 0.068, 'train_steps_per_second': 0.068, 'train_loss': 9.038271639082167, 'epoch': 3.0}\n",
      "âœ… Fine-tuning complete! Model ready for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# ===== STEP 1: TOKENIZATION SETUP =====\n",
    "# Set pad token for tokenizer (required for batching)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text examples for language modeling.\n",
    "    \n",
    "    Args:\n",
    "        examples: Dict with 'text' key containing raw text\n",
    "        \n",
    "    Returns:\n",
    "        Tokenized inputs with token IDs and attention masks\n",
    "    \"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)\n",
    "print(f\"âœ… Dataset tokenized: {len(tokenized_dataset)} examples\")\n",
    "\n",
    "# ===== STEP 2: DATA COLLATOR =====\n",
    "# Data collator for causal language modeling (next token prediction)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # False for causal LM (predict next token), True for masked LM\n",
    ")\n",
    "\n",
    "# ===== STEP 3: TRAINING ARGUMENTS =====\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi2-insurance\",           # Where to save model checkpoints\n",
    "    per_device_train_batch_size=1,            # Batch size (1 for consumer hardware)\n",
    "    num_train_epochs=3,                       # Number of training epochs\n",
    "    logging_steps=1,                          # Log metrics every N steps\n",
    "    save_strategy=\"no\",                       # Don't save intermediate checkpoints\n",
    "    fp16=False,                               # Disable mixed precision (avoid precision issues)\n",
    "    learning_rate=2e-4,                       # Learning rate for optimization\n",
    "    weight_decay=0.01,                        # L2 regularization\n",
    "    logging_dir=\"./logs\",                      # Directory for TensorBoard logs\n",
    "    evaluation_strategy=\"epoch\"                # Evaluate at the end of each epoch\n",
    ")\n",
    "\n",
    "# ===== STEP 4: INITIALIZE TRAINER =====\n",
    "trainer = Trainer(\n",
    "    model=peft_model,                         # The PEFT-wrapped model with LoRA adapters\n",
    "    args=training_args,                       # Training configuration\n",
    "    train_dataset=tokenized_dataset,          # Training data\n",
    "    data_collator=data_collator,              # Prepare batches for training\n",
    "    tokenizer=tokenizer                       # Tokenizer for special token handling\n",
    ")\n",
    "\n",
    "# ===== STEP 5: TRAIN THE MODEL =====\n",
    "print(\"\\nðŸš€ Starting fine-tuning on insurance claims data...\")\n",
    "trainer.train()\n",
    "print(\"âœ… Fine-tuning complete! Model ready for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bd982a",
   "metadata": {},
   "source": [
    "## 5. Training Setup & Execution\n",
    "\n",
    "### Tokenization and Training Configuration\n",
    "This step:\n",
    "1. **Tokenizes** the formatted text into token IDs the model understands\n",
    "2. **Configures training arguments** (learning rate, batch size, epochs, etc.)\n",
    "3. **Initializes the Trainer** with the PEFT model, dataset, and training config\n",
    "4. **Trains** the model on the insurance claims data\n",
    "\n",
    "**Note**: We use a batch size of 1 for memory efficiency on consumer hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae5ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text):\n",
    "    \"\"\"\n",
    "    Generate text using the fine-tuned Phi-2 model.\n",
    "    \n",
    "    Args:\n",
    "        text: Input prompt string\n",
    "        \n",
    "    Returns:\n",
    "        Generated text continuation\n",
    "    \"\"\"\n",
    "    # Detect the appropriate device for inference\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = \"mps\"  # Apple Metal Performance Shaders\n",
    "    elif torch.cuda.is_available():\n",
    "        device = \"cuda\"  # NVIDIA CUDA\n",
    "    else:\n",
    "        device = \"cpu\"  # CPU fallback\n",
    "    \n",
    "    # Tokenize the input prompt and move to the correct device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate text with the model in evaluation mode\n",
    "    peft_model.eval()\n",
    "    with torch.no_grad():  # Disable gradient computation for faster inference\n",
    "        output = peft_model.generate(\n",
    "            **inputs,\n",
    "            max_length=200,      # Maximum output length\n",
    "            do_sample=False      # Greedy decoding (deterministic)\n",
    "        )\n",
    "    \n",
    "    # Decode token IDs back to readable text\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# ===== EXAMPLE INFERENCE =====\n",
    "# Create a test prompt following the instruction-response format\n",
    "prompt = \"\"\"### Instruction:\n",
    "Explain denial for missing clinical documentation.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Generate response from the fine-tuned model\n",
    "print(\"ðŸ”„ Generating response from fine-tuned Phi-2 model...\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(generate(prompt))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e657b",
   "metadata": {},
   "source": [
    "## 6. Inference & Text Generation\n",
    "\n",
    "### Generate Responses with Fine-tuned Model\n",
    "The `generate()` function:\n",
    "1. **Detects hardware**: Uses MPS (Mac), CUDA (GPU), or CPU\n",
    "2. **Tokenizes input**: Converts the prompt to token IDs\n",
    "3. **Generates output**: Uses the fine-tuned model to generate responses\n",
    "4. **Decodes output**: Converts token IDs back to human-readable text\n",
    "\n",
    "The model is evaluated in `eval()` mode and uses `torch.no_grad()` for memory efficiency during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "âœ… Created an insurance claims dataset with instruction-response pairs  \n",
    "âœ… Loaded the Phi-2 model and applied LoRA adapters  \n",
    "âœ… Formatted and tokenized the data for training  \n",
    "âœ… Fine-tuned the model on domain-specific insurance data  \n",
    "âœ… Demonstrated inference with the trained model  \n",
    "\n",
    "### Key Concepts Covered\n",
    "- **Parameter-Efficient Fine-tuning**: LoRA reduces trainable parameters by 99%+\n",
    "- **Domain Adaptation**: Customizing a general-purpose model for insurance tasks\n",
    "- **Hardware Optimization**: Multi-device support (MPS/CUDA/CPU)\n",
    "- **Instruction-Response Formatting**: Structured prompts for better model behavior\n",
    "\n",
    "### Next Steps to Explore\n",
    "1. **Expand Dataset**: Collect more insurance claim examples for better generalization\n",
    "2. **Evaluate Performance**: Test on held-out validation data and calculate metrics\n",
    "3. **Fine-tune Hyperparameters**: Experiment with learning rate, rank, dropout\n",
    "4. **Save & Load Model**: Persist LoRA weights for production use\n",
    "5. **Deploy**: Create an API endpoint for real-time inference\n",
    "\n",
    "### Resources\n",
    "- [Phi-2 Model Card](https://huggingface.co/microsoft/phi-2)\n",
    "- [PEFT Library Documentation](https://github.com/huggingface/peft)\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
